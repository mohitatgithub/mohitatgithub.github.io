<!DOCTYPE html>
<html lang="en">
  <!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2016 -->
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Introduction to Language Models</title>

  <meta name="author" content="Mohit Rathore" />

  

  <link rel="alternate" type="application/rss+xml" title="Blog - Mohit Rathore" href="/feed.xml" />

  

  
    
      
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.0/css/font-awesome.min.css" />

    
  

  
    
      <link rel="stylesheet" href="/css/bootstrap.min.css" />
    
      <link rel="stylesheet" href="/css/bootstrap-social.css" />
    
      <link rel="stylesheet" href="/css/main.css" />
    
  

  
    
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
    
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
    
  

  

  

  

    <!-- Facebook OpenGraph tags -->
  

  
  <meta property="og:title" content="Introduction to Language Models" />
  

   
  <meta property="og:description" content="Language Modeling is an important idea behind many Natural Language Processing tasks such as Machine Translation, Spelling Correction, Speech Recognition, Summarization, Question-Answering etc. Some recent applications of Language models involve Smart Reply in Gmail &amp;amp; Google Text suggestion in SMS. This introduction is mostly inspired by Chapter 4 ‘Language Modeling...">
  


  <meta property="og:type" content="website" />

  
  <meta property="og:url" content="http://mohitatgithub.github.io/2018-04-03-Introduction-to-Language-Models/" />
  <link rel="canonical" href="http://mohitatgithub.github.io/2018-04-03-Introduction-to-Language-Models/" />
  

  


  <!-- Twitter summary cards -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@" />
  <meta name="twitter:creator" content="@" />

  
  <meta name="twitter:title" content="Introduction to Language Models" />
  

  
  <meta name="twitter:description" content="Language Modeling is an important idea behind many Natural Language Processing tasks such as Machine Translation, Spelling Correction, Speech Recognition, Summarization, Question-Answering etc. Some recent applications of Language models involve Smart Reply in Gmail &amp;amp; Google Text suggestion in SMS. This introduction is mostly inspired by Chapter 4 ‘Language Modeling...">
  

  

  

</head>


  <body>

    
  
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
        <a class="navbar-brand" href="http://mohitatgithub.github.io">Blog</a>
      
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
          <li>
            






<a href="/aboutme">About Me</a>

          </li>
        
        
      </ul>
    </div>

	

  </div>
</nav>


    <!-- TODO this file has become a mess, refactor it -->





<header class="header-section ">

<div class="intro-header no-img">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-heading">
          <h1>Introduction to Language Models</h1>
		  
		  
		  
		  <span class="post-meta">Posted on April 3, 2018</span>
		  
        </div>
      </div>
    </div>
  </div>
</div>
</header>




<div class="container">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

      

      <article role="main" class="blog-post">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async=""></script>

<p>Language Modeling is an important idea behind many Natural Language Processing tasks such as Machine Translation, Spelling Correction, Speech Recognition, Summarization, Question-Answering etc. Some recent applications of Language models involve Smart Reply in Gmail &amp; Google Text suggestion in SMS. This introduction is mostly inspired by Chapter 4 ‘Language Modeling with N-grams’ of book: “Speech and Language Processing” by Daniel Jurafsky &amp; James H. Marti <sup><a href="https://web.stanford.edu/~jurafsky/slp3/">[1]</a></sup></p>

<h3 id="what-is-a-language-model">What is a Language Model?</h3>

<p>Basically Language Model assigns a probability to a sentence or a sequence of words. so given a sequence of words
<script type="math/tex">w_1,w_2,w_3,w_4,w_5,...,w_n</script></p>

<p>Probability <script type="math/tex">P(W)</script> of this sequence of words will be:</p>

<h4 id="pw--pw_1w_2w_3w_4w_5w_n"><center>$$P(W) = P(w_1,w_2,w_3,w_4,w_5,...,w_n)$$</center></h4>

<p>We can also calculate probability of upcoming word <script type="math/tex">w_n</script> given previous words as:</p>

<h4 id="pw_n--pw_n--w_1w_2w_3w_4w_5w_n-1"><center>$$P(w_n) = P(w_n\, | \,w_1,w_2,w_3,w_4,w_5,...,w_{n-1})$$</center></h4>

<p>Lets take an example of sentence “weather is pleasant”. Probability of this sentence will be expressed as:</p>

<h4 id="pweather-is-pleasant--pweather-times-pis--weather-times-ppleasant--weather-is"><center>$$P("weather\, is\, pleasant") = P(weather)\, \times \,P(is\, | \,weather)\, \times \,P(pleasant\, | \,weather\, is)$$</center></h4>

<p>Formally this can be expressed as:</p>

<h4 id="pw_1w_2w_3w_4w_5w_n--prod_i1n-pw_i--w_1w_2w_3w_4w_5w_i-1"><center>$$P(w_1,w_2,w_3,w_4,w_5,...,w_n) = \prod_{i=1}^n\, P(w_i\, | \,w_1,w_2,w_3,w_4,w_5,...,w_{i-1})$$</center></h4>

<p>But how to calculate this probability?</p>

<p><br /></p>

<h3 id="markov-assumption">Markov Assumption</h3>

<p>Andrey Markov was a Russian mathematician who described a stochastic process with a property called Markov Property or Markov Assumption. This basically states that one can make predictions for the future of the process based solely on its present state just as well as one could knowing the process’s full history, hence independently from such history.<sup><a href="https://en.wikipedia.org/wiki/Markov_property">[2]</a></sup></p>

<p>Based on this assumption we can rewrite conditional probability of ‘pleasant’ as:</p>

<h4 id="ppleasant--weather-is-approx-ppleasant--weather"><center>$$P(pleasant\, | \,weather\, is) \approx P(pleasant\, | \,weather)$$</center></h4>

<p>Or formally as:</p>

<h4 id="pw_n--w_1w_2w_3w_4w_5w_n-1--pw_n--w_n-1"><center>$$P(w_n\, | \,w_1,w_2,w_3,w_4,w_5,...,w_{n-1}) = P(w_n\, | \,w_{n-1})$$</center></h4>

<p>Above equation also represent a Bigram<sup><a href="https://en.wikipedia.org/wiki/Bigram">[3]</a></sup> model. In case of N-gram models this assumption can be extended such that conditional probability may depend on couple of previous words as:</p>

<h4 id="pw_n--w_1w_2w_3w_4w_5w_n-1--pw_n--w_n-kw_n-1"><center>$$P(w_n\, | \,w_1,w_2,w_3,w_4,w_5,...,w_{n-1}) = P(w_n\, | \,w_{n-k},...,w_{n-1})$$</center></h4>

<p><br /></p>

<h3 id="bigram-model">Bigram Model</h3>

<p>Now lets find probability for simplest model where conditional probability of any word only depends on previous word by Markov assumption.</p>

<h4 id="pw_iw_i-1--fraccountw_i-1w_icountw_i-1"><center>$$P(w_i\,|\,w_{i-1}) = \frac{count(w_{i-1},w_i)}{count(w_{i-1})}$$</center></h4>

<p>Lets understand this with a small corpus of famous poem ‘A Girl’ by Ezra Pound<br /><br /></p>

<blockquote>
  <p>The tree has entered <strong>my hands</strong>,<br />
The sap has ascended <strong>my arms</strong>,<br />
The tree has grown in <strong>my breast-Downward</strong>,<br />
The branches grow out of me, like arms.<br /></p>
</blockquote>

<blockquote>
  <p>Tree you are,<br />
Moss you are,<br />
You are violets with wind above them.<br />
A child - so high - you are,<br />
And all this is folly to the world.<br /></p>
</blockquote>

<p>Here word ‘arms’ appears after ‘my’ only once but word ‘my’ appeared total three times in poem so Probability of ‘arms’ given ‘my’ is:</p>
<h4 id="parms--my--frac13"><center>$$P(arms\, |\, my) = \frac{1}{3}$$</center></h4>

<p>To calculate probability of first &amp; last word <code class="highlighter-rouge">&lt;s&gt;</code> &amp; <code class="highlighter-rouge">&lt;\s&gt;</code> are added at start &amp; end of sentence respectively. Similarly probability of sentence or sequence of words can be calculated using above approach by multiplying all Bigram probabilities.</p>

<p><br /></p>

<h3 id="applications-of-language-models">Applications of Language Models</h3>

<p><strong>1. Spelling Correction</strong></p>

<p>For spelling correction probability of incorrect sentence will be much smaller then correct sentence</p>

<h4 id="pweather-is-mathbfpleasent--pweather-is-mathbfplesen"><center>$$P(weather\, is\, \mathbf{pleasent}) &gt; P(weather\, is\, \mathbf{plesen})$$</center></h4>

<p><strong>2. Speech Recognition</strong></p>

<p>As words ‘weather’ &amp; ‘whether’ may have similar phonetics, system may confuse among these but probability of ‘weather is pleasant’ will be higher than ‘whether is pleasant’</p>

<h4 id="pmathbfweather-is-pleasant--pmathbfwhether-is-pleasant"><center>$$P(\mathbf{weather}\, is\, pleasant) &gt; P(\mathbf{whether}\, is\, pleasant)$$</center></h4>

<p><strong>3. Machine Translation</strong></p>

<p>Selecting appropriate sequence while translating from one language to another can also use probability of sequence for providing more appropriate translations</p>

<h4 id="pmathbfhigh-winds-tonight--pmathbflarge-winds-tonight"><center>$$P(\mathbf{high}\, winds\, tonight) &gt; P(\mathbf{large}\, winds\, tonight)$$</center></h4>

<p><strong>4. Gmail Smart Reply</strong></p>

<p>Complete details of this system are discussed in this paper: “Smart Reply: Automated Response Suggestion for Email”   <sup><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45189.pdf">[4]</a></sup></p>

<p><img src="/Images/Blog/Language_Models/smart_reply.png" /></p>

<center>
<a href="https://blog.google/products/gmail/save-time-with-smart-reply-in-gmail/">Image Source</a>
</center>

<p><strong>5. Predictive Text</strong></p>

<p>By looking at previous sequence of words language model can predict next word, this feature was recently introduced in android phone keyboard by Google.
<br /><br /></p>

<center><img src="/Images/Blog/Language_Models/predictive_text.png" style="width:460px;height:190px;" /></center>

<p>Now lets build our own language model for predicting next word using a very small corpus of words. I will build this model using keras &amp; deep neural network architecture with LSTM.</p>

<p><br /></p>

<h3 id="building-language-model">Building Language Model</h3>

<p>As Language modeling involves predicting the next word in a sequence given the sequence of words already present we can train a language model to create subsequent words in sequence from given starting sequence.</p>

<p>Sequence Models or Recurrent neural networks, or RNNs<sup><a href="http://www.dtic.mil/get-tr-doc/pdf?AD=ADA164453">[5]</a></sup> are a family of neural networks for processing sequential data. A detailed discussion on Sequence Models is provided in Chapter 10 of Deep Learning Book. <sup><a href="http://www.deeplearningbook.org/contents/rnn.html">[6]</a></sup></p>

<p>Language model design discussed below is inspired by this insightful article: <a href="https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/">How to Develop Word-Based Neural Language Models in Python with Keras</a> by Jason Brownlee.</p>

<p>For this language model I will use part of the poem “This Is the House That Jack Built” <sup><a href="https://en.wikipedia.org/wiki/This_Is_the_House_That_Jack_Built">[7]</a></sup> as training corpus.</p>

<p><strong><em>This Is the House That Jack Build</em></strong></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="s">""" This is the house that Jack built.</span><span class="se">\n</span><span class="s">
This is the malt that lay in the house that Jack built.</span><span class="se">\n</span><span class="s">
This is the rat that ate the malt</span><span class="se">\n</span><span class="s">
That lay in the house that Jack built.</span><span class="se">\n</span><span class="s">
This is the cat that killed the rat</span><span class="se">\n</span><span class="s">
That ate the malt that lay in the house that Jack built.</span><span class="se">\n</span><span class="s">
This is the dog that worried the cat</span><span class="se">\n</span><span class="s">
That killed the rat that ate the malt</span><span class="se">\n</span><span class="s">
That lay in the house that Jack built.</span><span class="se">\n</span><span class="s">
This is the cow with the crumpled horn</span><span class="se">\n</span><span class="s"> """</span>
</code></pre>
</div>

<p><em>Importing Packages</em></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span>
<span class="kn">from</span> <span class="nn">keras.utils.vis_utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
<span class="kn">from</span> <span class="nn">keras_tqdm</span> <span class="kn">import</span> <span class="n">TQDMNotebookCallback</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'TF_CPP_MIN_LOG_LEVEL'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'3'</span>
</code></pre>
</div>

<p><em>Procedure to generate sequence</em></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">n_words</span><span class="p">):</span>
	<span class="n">in_text</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">seed_text</span>
	<span class="c"># generate a fixed number of words</span>
	<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_words</span><span class="p">):</span>
		<span class="c"># encode the text as integer</span>
		<span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">in_text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
		<span class="n">encoded</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
		<span class="c"># predict a word in the vocabulary</span>
		<span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_classes</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
		<span class="c"># map predicted word index to word</span>
		<span class="n">out_word</span> <span class="o">=</span> <span class="s">''</span>
		<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
			<span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">yhat</span><span class="p">:</span>
				<span class="n">out_word</span> <span class="o">=</span> <span class="n">word</span>
				<span class="k">break</span>
		<span class="c"># append to input</span>
		<span class="n">in_text</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="n">out_word</span><span class="p">,</span> <span class="n">result</span> <span class="o">+</span> <span class="s">' '</span> <span class="o">+</span> <span class="n">out_word</span>
	<span class="k">return</span> <span class="n">result</span>
</code></pre>
</div>

<p><em>Integer encoding of text</em></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">([</span><span class="n">data</span><span class="p">])</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">data</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">" "</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Word to Interger mapping of first 6 words:</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">encoded</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Word to Interger mapping of first 6 words:

This 3
is 4
the 1
house 5
that 2
Jack 6
</code></pre>
</div>

<p><em>Determining the vocabulary size</em></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Vocabulary Size: </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="n">vocab_size</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Vocabulary Size: 21
</code></pre>
</div>

<p><em>Creating sequences of words to fit the model with one word as input and one word as output</em></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">sequences</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">)):</span>
	<span class="n">sequence</span> <span class="o">=</span> <span class="n">encoded</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
	<span class="n">sequences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Total Sequences: </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'First 10 Sequences: '</span><span class="p">,</span><span class="n">sequences</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Total Sequences: 86
First 10 Sequences:  [[3, 4], [4, 1], [1, 5], [5, 2], [2, 6], [6, 7], [7, 3], [3, 4], [4, 1], [1, 8]]
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">sequences</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">sequences</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"First 5 Sequence Outputs"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Their One Hot encodings:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>First 5 Sequence Outputs
[4 1 5 2 6]

Their One Hot encodings:
[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
</code></pre>
</div>

<p><em>Building Model</em></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">50</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
</code></pre>
</div>

<p><em>Model Architecture</em></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">to_file</span><span class="o">=</span><span class="s">'model_plot.png'</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">show_layer_names</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 1, 10)             210       
_________________________________________________________________
lstm_1 (LSTM)                (None, 50)                12200     
_________________________________________________________________
dense_1 (Dense)              (None, 21)                1071      
=================================================================
Total params: 13,481
Trainable params: 13,481
Non-trainable params: 0
_________________________________________________________________
None
</code></pre>
</div>

<center><img src="/Images/Blog/Language_Models/model_plot.png" /></center>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                          <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                          <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">TQDMNotebookCallback</span><span class="p">()])</span>
</code></pre>
</div>

<p><em>Finally lets generate sequence of 6 words by providing only first letter for sequence</em></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">generate_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="s">'This'</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>This is the house that jack built
</code></pre>
</div>

<p>As expected the model generated the most plausible line based on training corpus.</p>

<p><em>Accuracy &amp; Loss Plots</em></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'model accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'test'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="/Images/Blog/Language_Models/output_36_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'model loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'test'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="/Images/Blog/Language_Models/output_37_0.png" alt="png" /></p>

<h3 id="refrences">Refrences</h3>

<ol>
  <li>Jurafsky, Dan. Speech &amp; language processing. Pearson Education India, 2000.</li>
  <li>https://en.wikipedia.org/wiki/Markov_property</li>
  <li>https://en.wikipedia.org/wiki/Bigram</li>
  <li>Kannan, Anjuli, et al. “Smart reply: Automated response suggestion for email.” Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.</li>
  <li>Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations by error propagation. No. ICS-8506. California Univ San Diego La Jolla Inst for Cognitive Science, 1985.</li>
  <li>https://en.wikipedia.org/wiki/This_Is_the_House_That_Jack_Built</li>
  <li>http://www.deeplearningbook.org/contents/rnn.html</li>
  <li>https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/</li>
</ol>

<p><em>Thanks for reading this notebook. I am learning NLP &amp; not an expert in this field, feel free to provide your feedback on errors &amp; improvements.</em></p>

      </article>

      
        <div class="blog-tags">
          Tags:
          
          
            <a href="/tags#NLP">NLP</a>
          
            <a href="/tags#Language Modeling">Language Modeling</a>
          
            <a href="/tags#Deep Learning">Deep Learning</a>
          
            <a href="/tags#Tutorial">Tutorial</a>
          
            <a href="/tags#Machine Learning">Machine Learning</a>
          
            <a href="/tags#Keras">Keras</a>
          
            <a href="/tags#Python">Python</a>
          
          
        </div>
      

      
        <!-- Check if any share-links are active -->




<section id = "social-share-section">
  <span class="sr-only">Share: </span>

  
  <!--- Share on Twitter -->
    <a href="https://twitter.com/intent/tweet?text=Introduction+to+Language+Models+http://mohitatgithub.github.io/2018-04-03-Introduction-to-Language-Models/"
      class="btn btn-social-icon btn-twitter" title="Share on Twitter">
      <span class="fa fa-fw fa-twitter" aria-hidden="true"></span>
      <span class="sr-only">Twitter</span>
    </a>
  

  

  
  <!--- Share on Google Plus -->
    <a href="https://plus.google.com/share?url=http://mohitatgithub.github.io/2018-04-03-Introduction-to-Language-Models/"
      class="btn btn-social-icon btn-google" title="Share on Google+">
      <span class="fa fa-fw fa-google-plus" aria-hidden="true"></span>
      <span class="sr-only">Google+</span>
    </a>
  

  
  <!--- Share on LinkedIn -->
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://mohitatgithub.github.io/2018-04-03-Introduction-to-Language-Models/"
      class="btn btn-social-icon btn-linkedin" title="Share on LinkedIn">
      <span class="fa fa-fw fa-linkedin" aria-hidden="true"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  

</section>



      

      <ul class="pager blog-pager">
        
        <li class="previous">
          <a href="/2018-03-28-MNIST-Image-Classification-with-CNN-&-Keras/" data-toggle="tooltip" data-placement="top" title="MNIST image classification with CNN & Keras">&larr; Previous Post</a>
        </li>
        
        
        <li class="next">
          <a href="/2018-04-28-Learning-tf-idf-with-tidytext/" data-toggle="tooltip" data-placement="top" title="Understanding tf-idf with tidytext">Next Post &rarr;</a>
        </li>
        
      </ul>

      
        <div class="disqus-comments">
          <div class="comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'mohitrathore';
        /* ensure that pages with query string get the same discussion */
            var url_parts = window.location.href.split("?");
            var disqus_url = url_parts[0];
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
        </div>
      
    </div>
  </div>
</div>


    <footer>
  <div class="container beautiful-jekyll-footer">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links"><li><a href="/feed.xml" title="RSS"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">RSS</span>
              </a>
            </li><li><a href="mailto:mohitr1729@gmail.com" title="Email me"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">Email me</span>
              </a>
            </li><li><a href="https://github.com/mohitatgithub" title="GitHub"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">GitHub</span>
              </a>
            </li><li><a href="https://linkedin.com/in/mohitr1729" title="LinkedIn"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">LinkedIn</span>
              </a>
            </li><li><a href="https://www.kaggle.com/mohitr" title="Kaggle"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-trophy fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">Kaggle</span>
              </a>
            </li><li><a href="https://www.quora.com/Mohit-Rathore-6" title="Quora"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-question fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">Quora</span>
              </a>
            </li></ul>
      <p class="copyright text-muted">
      Mohit Rathore
      &nbsp;&bull;&nbsp;
      2020

      

      
      </p>
          <!-- Please don't remove this, keep my open source work credited :) -->
    <p class="theme-by text-muted">
      Theme by
      <a href="http://deanattali.com/beautiful-jekyll/">beautiful-jekyll</a>
    </p>
      </div>
    </div>
  </div>
</footer>

  
    






  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script>
      	if (typeof jQuery == 'undefined') {
      	  document.write('<script src="/js/jquery-1.11.2.min.js"></scr' + 'ipt>');
      	}
      </script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="/js/bootstrap.min.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="/js/main.js"></script>
    
  



	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-114904532-1', 'auto');
		ga('send', 'pageview');
	</script>
	<!-- End Google Analytics -->


  
  </body>
</html>
